{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a12b03fa",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "- GitHub: \n",
    "\n",
    "https://github.com/hwchase17/langchain\n",
    "\n",
    "- Docs: \n",
    "\n",
    "https://python.langchain.com/en/latest/\n",
    "\n",
    "\n",
    "- Features:\n",
    "        Interface LLMs\n",
    "        Prompt Templates\n",
    "        Chains (Construct sequences of calls)\n",
    "        Agents and Tools\n",
    "        Memory (Persist application state between runs of a chain)\n",
    "        Document Loaders\n",
    "        Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71dfb26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install openai\n",
    "# !pip install huggingface_hub\n",
    "# !pip install wikipedia\n",
    "# !pip install sentence_transformers\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3a38fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691881bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://platform.openai.com/account/api-keys\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "# https://huggingface.co/settings/tokens\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76320986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anthropic',\n",
       " 'ArxivAPIWrapper',\n",
       " 'Banana',\n",
       " 'BaseCache',\n",
       " 'BasePromptTemplate',\n",
       " 'CerebriumAI',\n",
       " 'Cohere',\n",
       " 'ConversationChain',\n",
       " 'ElasticVectorSearch',\n",
       " 'FAISS',\n",
       " 'FewShotPromptTemplate',\n",
       " 'ForefrontAI',\n",
       " 'GoogleSearchAPIWrapper',\n",
       " 'GoogleSerperAPIWrapper',\n",
       " 'GooseAI',\n",
       " 'HuggingFaceHub',\n",
       " 'HuggingFacePipeline',\n",
       " 'HuggingFaceTextGenInference',\n",
       " 'InMemoryDocstore',\n",
       " 'LLMBashChain',\n",
       " 'LLMChain',\n",
       " 'LLMCheckerChain',\n",
       " 'LLMMathChain',\n",
       " 'LlamaCpp',\n",
       " 'MRKLChain',\n",
       " 'Modal',\n",
       " 'OpenAI',\n",
       " 'Optional',\n",
       " 'PALChain',\n",
       " 'Petals',\n",
       " 'PipelineAI',\n",
       " 'PowerBIDataset',\n",
       " 'Prompt',\n",
       " 'PromptTemplate',\n",
       " 'QAWithSourcesChain',\n",
       " 'ReActChain',\n",
       " 'SQLDatabase',\n",
       " 'SQLDatabaseChain',\n",
       " 'SagemakerEndpoint',\n",
       " 'SearxSearchWrapper',\n",
       " 'SelfAskWithSearchChain',\n",
       " 'SerpAPIChain',\n",
       " 'SerpAPIWrapper',\n",
       " 'StochasticAI',\n",
       " 'VectorDBQA',\n",
       " 'VectorDBQAWithSourcesChain',\n",
       " 'Wikipedia',\n",
       " 'WikipediaAPIWrapper',\n",
       " 'WolframAlphaAPIWrapper',\n",
       " 'Writer',\n",
       " '__all__',\n",
       " '__annotations__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'agents',\n",
       " 'base_language',\n",
       " 'cache',\n",
       " 'callbacks',\n",
       " 'chains',\n",
       " 'chat_models',\n",
       " 'debug',\n",
       " 'docstore',\n",
       " 'document_loaders',\n",
       " 'embeddings',\n",
       " 'env',\n",
       " 'formatting',\n",
       " 'graphs',\n",
       " 'input',\n",
       " 'llm_cache',\n",
       " 'llms',\n",
       " 'load',\n",
       " 'math_utils',\n",
       " 'memory',\n",
       " 'output_parsers',\n",
       " 'prompts',\n",
       " 'requests',\n",
       " 'schema',\n",
       " 'sql_database',\n",
       " 'text_splitter',\n",
       " 'tools',\n",
       " 'utilities',\n",
       " 'utils',\n",
       " 'vectorstores',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(langchain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24376f77",
   "metadata": {},
   "source": [
    "## 1. LLMs\n",
    "A generic interface for all LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e55a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f606dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmartMatch Solutions\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI()\n",
    "\n",
    "text = \"What would be a good company name for a company that provides AI based recommendation and search engines?\"\n",
    "res = llm(text)\n",
    "\n",
    "print(res.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b52d401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e918909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
    "llm(\"translate English to German: How old are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590e390",
   "metadata": {},
   "source": [
    "## 2. Prompt Templates\n",
    "\n",
    "LangChain faciliates prompt management and optimization.\n",
    "\n",
    "Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32b7ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "085a33be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, it is not possible for Barack Obama to have a conversation with George Washington, as George Washington passed away in 1799, more than 200 years before Barack Obama was born.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Can Barack Obama have a conversation with George Washington?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32c511cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c14386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nQuestion: Can Mahatma Gandhi have a conversation with Sachin Tendulkar?\\n\\nLet's think step by step.\\n\\nAnswer:\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "prompt.format(question=\"Can Mahatma Gandhi have a conversation with Sachin Tendulkar?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe86b023",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `prompt` is expected to be a string. Instead found <class 'langchain.prompts.prompt.PromptTemplate'>. If you want to run the LLM on multiple prompts, use `generate` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4004\\3080619261.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mllm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# TypeError: Object of type PromptTemplate is not JSON serializable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;34m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    422\u001b[0m                 \u001b[1;34m\"Argument `prompt` is expected to be a string. Instead found \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                 \u001b[1;34mf\"{type(prompt)}. If you want to run the LLM on multiple prompts, use \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Argument `prompt` is expected to be a string. Instead found <class 'langchain.prompts.prompt.PromptTemplate'>. If you want to run the LLM on multiple prompts, use `generate` instead."
     ]
    }
   ],
   "source": [
    "llm(prompt)  # TypeError: Object of type PromptTemplate is not JSON serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa1c617",
   "metadata": {},
   "source": [
    "## 3. Chains\n",
    "Combine LLMs and Prompts in multi-step workflowsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c36298b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89a11bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, Barack Obama cannot have a conversation with George Washington. George Washington died in 1799, so it would be impossible to have a conversation with him.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Can Barack Obama have a conversation with George Washington?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9e3baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colorful Socks Co.\n"
     ]
    }
   ],
   "source": [
    "# If there are multiple variables, you can input them all at once using a dictionary.\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"What is a good name for {company} that makes {product}?\",\n",
    "    input_variables=[\"company\", \"product\"],\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run({\n",
    "    'company': \"ABC Startup\",\n",
    "    'product': \"colorful socks\"\n",
    "    }).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957a9ce",
   "metadata": {},
   "source": [
    "## 4. Agents and Tools\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
    "\n",
    "When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n",
    "\n",
    "    Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
    "    LLM: The language model powering the agent.\n",
    "    Agent: The agent to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994f234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools , initialize_agent\n",
    "from langchain.llms import OpenAI     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ef8b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model='text-davinci-001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "768fe46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "747ca74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"In what year was the film 3 Idiots released? What is this release year raised to the 2 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc26effd",
   "metadata": {},
   "source": [
    "## 5. Memory\n",
    "Add State to Chains and Agents.\n",
    "\n",
    "Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e410ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "722dd70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74eecdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: My name is Gautam. What do you think about Indian Premiere League?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi Gautam! It's nice to meet you. I think the Indian Premier League is a great way to bring cricket fans together and create a sense of community. It's also a great way to promote the sport and bring in new fans.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"My name is Gautam. What do you think about Indian Premiere League?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11307f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: My name is Gautam. What do you think about Indian Premiere League?\n",
      "AI:  Hi Gautam! It's nice to meet you. I think the Indian Premier League is a great way to bring cricket fans together and create a sense of community. It's also a great way to promote the sport and bring in new fans.\n",
      "Human: Which team do you think has the most loyal fanbase in this league ?\n",
      "It is known to win hearts everytime and has qualified many times in playoffs bt never won any trophy\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's a tough question! I don't have enough information to answer that definitively, but I do know that the Chennai Super Kings have a very loyal fanbase. They have been in the playoffs many times and have come close to winning the trophy, but have never been able to take it home.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"\"\"Which team do you think has the most loyal fanbase in this league ?\n",
    "It is known to win hearts everytime and has qualified many times in playoffs bt never won any trophy\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28426695",
   "metadata": {},
   "source": [
    "## 6. Document Loaders\n",
    "Combining language models with your own text data is a powerful way to differentiate them. The first step in doing this is to load the data into “documents” - a fancy way of say some pieces of text. This module is aimed at making this easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a7a52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader , TextLoader\n",
    "loader = NotionDirectoryLoader(\"Notion_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2325ba48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046bf7cf",
   "metadata": {},
   "source": [
    "## 7. Indexes\n",
    "Indexes refer to ways to structure documents so that LLMs can best interact with them. This module contains utility functions for working with documents\n",
    "\n",
    "- Embeddings: An embedding is a numerical representation of a piece of information, for example, text, documents, images, audio, etc.\n",
    "- Text Splitters: When you want to deal with long pieces of text, it is necessary to split up that text into chunks.\n",
    "- Vectorstores: Vector databases store and index vector embeddings from NLP models to understand the meaning and context of strings of text, sentences, and whole documents for more accurate and relevant search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9bfb0c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned sold'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/extras/modules/state_of_the_union.txt\"\n",
    "res = requests.get(url)\n",
    "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "    f.write(res.text)\n",
    "\n",
    "    \n",
    "# Document Loader\n",
    "loader = TextLoader('./state_of_the_union.txt')\n",
    "documents = loader.load()\n",
    "documents[0].page_content[:1050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c04d9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "def31deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.', metadata={'source': './state_of_the_union.txt'})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a24da818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "doc_result = embeddings.embed_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb4ab992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db70305f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \n",
      "\n",
      "I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \n",
      "\n",
      "They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n",
      "\n",
      "Officer Mora was 27 years old. \n",
      "\n",
      "Officer Rivera was 22. \n",
      "\n",
      "Both Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \n",
      "\n",
      "I’ve worked on these issues a long time. \n",
      "\n",
      "I know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.\n"
     ]
    }
   ],
   "source": [
    "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "query = \"Where did the scandal happened\"\n",
    "res = db.similarity_search(query)\n",
    "print(res[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50e133d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "848b61f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \n",
      "\n",
      "I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \n",
      "\n",
      "They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n",
      "\n",
      "Officer Mora was 27 years old. \n",
      "\n",
      "Officer Rivera was 22. \n",
      "\n",
      "Both Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \n",
      "\n",
      "I’ve worked on these issues a long time. \n",
      "\n",
      "I know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.\n"
     ]
    }
   ],
   "source": [
    "new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "res = new_db.similarity_search(query)\n",
    "print(res[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b7343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
